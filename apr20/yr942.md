---
title: Applications: Machine Learning
author: Yafu Ruan <yr942@nyu.edu>
---
# Introduction
Those are brief introductions for this week's papers:
1. PyTorch Distributed presents the design, implementation, and evaluation of the distributed data parallel package in PyTorch v1.5
2. TVM introduces a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends
3. Clippers introduces a modular architecture to simplify model deployment across frameworks and applications.




# PyTorch Distributed
## Motivation
The development of deep learning model makes the datasets and models larger than before, which require more resources on training process. For DNN, there are three main operations: the forward pass to compute loss, the backward pass to compute gradients, and the optimizer step to update parameters.

Applications and make replicas of training data, and each of them is working on portion of the whole dataset and do forward/backward operations. As a result, the loss and gradient could be synchronized from different resources like calculating the sum average.

## Approaches

PyTorch uses DistributedDataParallel(DDP) to parallelize data and models across different machines and processors. It contains a Python API frontend, C++ gradient reduction core algorithm, and employs the c10d collective communication library as showns below:

![ddp](images/ddp.png)

 1. Python API
 Theere are two main design goals for API:
 - Non-intrusive: The developer should able to make minimal change to codes when transfering from local to parallel computing.
 - Interceptive: The API needs to allow the implementation to intercept various signals and trigger appropriate algorithms promptly.

 Below indicates that only one line needed when we transfer local to distributed machines.

 ![ddp](images/ddp.png)

 2. Gradient Reduction

 - Naive solution: PyTorch autograd engine accepts custom backward hooks. DDP can register autograd hooks to trigger computation after every backward pass. When fired, each hook scans through all local model parameters, and retrieves the gradient tensor from each parameter. Then, it uses the AllReduce collective communication call to calculate the average gradients on each parameter across all processes, and writes the result back to the gradient tensor.

 - Gradient Bucketing: Experients shows that the total communication time decreases when using larger input tensors. Instead of launching a dedicated AllReduce immediately when each gradient ten- sor becomes available, DDP can achieve higher throughput and lower latency if it waits for a short period of time and buckets multiple gradients into one AllReduce operation.

 - Overlap Computation with Communication: 

 - Gradient Accumulation: Here AllReduce is not called in each iteration. Instead, the application could do n iterations before gradient synchronizing.

  ![ddp](images/dgr.png)

 3. Collective Communication: DDP is built on top of collective communication libraries, including three options, NCCL, Gloo, and MPI. DDP takes the APIs from the three libraries and wraps them into the same ProcessGroup API.





## Trade-Offs

- No single configuration could work well for every job. Developers need to make individual configurations. Sometimes it also takes time to find out the optimal solution.


## Open Questions and Future Work

- Drop out technique:

As disscussed in the paper, drop out is a popular technique to avoid overfitting and acceleration, especially in deep neural network. For local environment, it is easy to accomplish. However, during DDP it could not save time since fixed parameter-to-bucket mapping. Consequently, regardless of how the forward pass skips layers, there is always the same amount of data to be communicated across the wire during the backward pass.

- Compressoin: By compressing the gradient, the volume of data during communication could be much less.



# TVM
## Motivation

Currently there are various of hardware accelerators and we need to rebuild the entire software stack on top of it for each of them. So it't better if we could design a bridge between the user's customized framework and the hardware.

## Approaches

Below shows the whole design of TVM:

![tvm](images/tvm.png)

- The system take input model from framework and transform it into computational graph.
- Then it uses high-level graph rewritting to generate a optomized computational graph.
- The operator-level optimization module must generate efficient code for each fused operator in this graph: TVM produces efficient code for each operator by generating many valid implementations on each hardware back-end and choosing an optimized implementation.
- We use use an ML-based cost model to find optimized operators.
- The system packs the generated code into a deployable module.

## Trade-Offs

## Open Questions and Future Work



# Clipper
## Motivation



## Approaches

## Trade-Offs

## Open Questions and Future Work

# References:
1. https://arxiv.org/abs/2006.15704



