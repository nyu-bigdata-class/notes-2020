---
title: "Programming Models"
author: Safwan Mahmood  <sm9453@nyu.edu>
---
# Introduction
A brief look into Programming Models:
* Naiad: A Timely Dataflow System
* Weld: Evaluating End-to-End Optimization for Data Analytics Applications
* Noria: Dynamic, partially-stateful data-flow for high-performance web applications

# Naiad: A Timely Dataflow System

## Introduction
Naiad is a distributed system for executing data parallel, cyclic dataflow programs. It offers the high throughput
of batch processors, the low latency of stream processors, and the ability to perform iterative and incremental
computations in a single framework. A new computational model, ***timely dataflow***, with timestamps that represent
logical points in the computation, underlies Naiad and captures opportunities for parallelism across a wide class of algorithms. 

## Motivation
Many data processing tasks require low-latency interactive access to results, iterative sub-computations, and
consistent intermediate outputs so that sub-computations can be nested and composed (Example: Iterative Machine Learning algorithms). However, no existing system satisfies all these requirements in a single sysytem under one hood. 
For example: 
1. Stream processors can produce low-latency results for non-iterative algorithms, 
2. Batch systems can iterate synchronously at the expense of latency.
3. Trigger-based approaches support iteration with only weak consistency guarantees. 

An applications built on a single platform with these features is typically more efficient, succinct, and maintainable. Naiad aims to fill this void with a general-purpose system that fulfills all of these requirements and supports a wide variety of high-level programming models. 

## Approaches
Timely Dataflow supports directed dataflow graphs with structured cycles, analogous to structured loops in a standard imperative programming language. This structure provides information about where records might possibly flow in the computation, allowing an implementation like Naiad to efficiently track and inform dataflow vertices about the possibility of additional records arriving at given streaming epochs or iterations.

Naiad developed a new computational model as mentioned above, timely dataflow, that supports the following features:
1. Structured loops allowing feedback in the dataflow. 
2. Stateful dataflow vertices capable of consuming and producing records without global coordination.

***(The above two features are needed to execute iterative and incremental computations with low latency.)***

3. Notifications for vertices once they have received all records for a given round of input or loop iteration.

***(This feature makes it possible to produce consistent results, at both outputs and intermediate stages of computations, in the presence of streaming or iteration.)***

***Terminologies:***

***Timely dataflow:*** Computational model based on a directed graph in which stateful vertices send and receive
logically timestamped messages along directed edges. Has cycles along with timestamps to distinguish data that arise in different input epochs and loop iterations.

***Input vertice:** Each input vertice receives a sequence of messages from an external producer.

***Output vertice:*** Each output emits a sequence of messages back to an external consumer. Each output message is labeled with its epoch.

***Loop contexts:*** Timestamp ordering with epochs and loop iterations is achieved using Inegress, Egress, and feedback vertices.

***Flow:***
The model supports both asynchronous and fine grained synchronous events execution.
Messages in a timely dataflow system flow only along edges, and their timestamps are modified by ingress, egress, and feedback vertices. All events and messages are asynchronous.

Vertices send and receive timestamped messages, and may request and receive notification that they have received all messages bearing a specific timestamp using two callbacks v.ONRECV(e : Edge, m : Message, t : Timestamp) and v.ONNOTIFY(t : Timestamp).

A vertex may invoke two system-provided methods in the context of these callbacks: this.SENDBY(e : Edge, m : Message, t : Timestamp) and this.NOTIFYAT(t : Timestamp).

Each call to u.SENDBY(e,m,t) results in a corresponding invocation of v.ONRECV(e,m,t), where e is an edge from u to v, and each call to v.NOTIFYAT(t) results in a corresponding invocation of v.ONNOTIFY(t).

The ONRECV and ONNOTIFY methods may contain arbitrary code and modify arbitrary per-vertex state, but do have an important constraint on their execution: when invoked with a timestamp t, the methods may only call
SENDBY or NOTIFYAT with timestamp t′ ≥ t. This rule guarantees that messages are not sent “backwards in time”.

SendBy and NotifyAt are API calls available to a data-flow vertex implementation to emit output records (SendBy) and to tell the runtime system that the vertex would like to receive a callback at a certain time (NotifyAt). It's the runtime system's job to figure, using the progress tracking protocol, when it is appropriate to invoke NotifyAt.

Fine grained synchronous events execution uses ONNOTIFY(t) calls to perform aggregations at timestamp t. 

To handle consistency of inside loop iterations and the timestamps of notifications, Naiad uses ***Progress Tracking Protocol.***(refer paper for details).


 ***Distributed implementation:***



## Tradeoffs 
Practical challenges arise when supporting applications that demand a mix of high-throughput and lowlatency computation. These challenges include coordinating distributed processes with low overhead, and
engineering the system to avoid stalls—from diverse
sources such as lock contention, dropped packets, and
garbage collection—that disproportionately affect computations that coordinate frequently

Updating large amounts of shared states, uses approximate states. Requires reliable network and overhead of sending these messages. 

Fault tolerance: Using rollback and checkpoint. Current design favors performance in the common case
that there are no failures, at the expense of availability in the event of a failure

Doesn't provide solutions to stragglers and load balancing across the workers.

Checkpointing stops the data processing and computations. Even logging was suggested but replaying logs while restoring has signficant overhead due to larger state info along with taking some damage on throughput. 

Faces challenges handling larger size of states due to runtime overheads and intervenes with the recovery mechanism, ie. 
checkpointing. 

Use of TCP might provide reliablilty but the bursty pattern of messages can lead to micro-stragglers
and adversely affect latencies.

There is no architectural reason why Naiad would ever be slower than Spark. However, Spark perhaps has a better story on interactive queries. In Spark, they can use existing, cached in-memory RDDs, while Naiad would require the user to recompile the program and run it again. Spark is also quite well-integrated with the Hadoop ecosystem and other systems that people already use (e.g., Python statistics libraries, cluster managers like YARN and Mesos), while Naiad is largely a standalone system.

Timely dataflow is in a bit of a weird space between language library and runtime system.(Ex: It doesn't quite have the stability guarantees a library might have (when you call data.sort() you don't think about "what if it fails?"))

***Paper Link:***
https://cs.nyu.edu/~apanda/classes/sp21/papers/naiad.pdf

## Open Questions and Future Work
Provide a robust fault tolerance system. Use of dynamic recovery mechanism at runtime, which can switch between logging and checkpointing. 
Handling large states and recoveries, deal with checkpoint latencies.
On going work on investigating the use of RDMA, which has the potential to reduce micro-stragglers due to TCP.
Naiad, still uses a mark-and-sweep garbage collector (GC) to reclaim memory instead of copy collection. It also kind of bends around to reduce GC calls to maintain low latency levels.
C# was chosen as a memory-safe language, made development easier but they actually had to do a fair amount of optimization work to address performance problems with C# on the critical data processing path (example: garbage collection). (Note: It was re-implemented in Rust).
Hardly used in industry due to non robust fault tolerance, gives flexibilty but gives too much work at hands of developer.
