---
title: "Reinforcement Learning"
author: Aakriti Talwar <at4793@nyu.edu>
---
# REINFORCEMENT LEARNING

# Introduction

# WHAT IS MENGER ?

Menger - a massive large-scale distributed RL infrastructure with localized inference that scales up to several thousand actors across multiple processing clusters (e.g., Borg cells), reducing the overall training time in the task of chip placement.

## WHAT IS ORLEAN?

Orleans is a cross-platform software framework for building scalable and robust distributed interactive applications based on the .NET Framework.

## WHAT IS RAY ?

- Ray is a cluster-computing framework that enables simulation, training, and serving for RL applications
- Ray implements a unified interface that can express both task-parallel and actorbased computations
- Tasks enable Ray to efficiently and dynamically load balance simulations, process large inputs and state spaces
- Actors enable Ray to efficiently support stateful computations, such as model training, and expose shared mutable state to clients

## WHAT IS ACME ?

Acme is a tool to simplify the development of novel RL algorithms that is specifically designed to enable simple agent implementations that can be run at various scales of execution. Its aim is to make the results of various RL algorithms
developed in academia and industrial labs easier to reproduce and extend.

## TERMINOLOGIES 

## MENGER

1. Replay buffer service - is a dedicated data storage for a collection of actors, generally located on the same Borg cells

## ACME 

1. Learning agent - an entity that perceives and acts— interacting with an unknown environment
2. Learners - processes which consume data in order to update policy parameters
3. ONline RL - vast amount of training data available for learning of the system
4. Offline RL - need to learn from a fixed dataset for leaning of the system

## RAY

1. Reinforcement learning - RL deals with learning to operate continuously within an uncertain environment based
on delayed and limited feedback

## ORLEAN

1. Activations - Orleans’ runtime automatically creates in-memory instances of an actor called activations.

# Motivation

## Menger 

- RL infrastructure is a loop of data collection and training, where actors explore the environment and collect samples, which are then sent to the learners to train and update the model.

## RAY - 

- RL infrastructure should not only scale efficiently but also be able to swiftly iterate over extensive amounts of samples during training.
- To learn a policy agent interacts with environment to generate trajectories, where a trajectory consists of a sequence of (state, reward) - used to improve the policy.
- RL systems must support training, serving, and simulation as shown in figure R1
  --- figure1

1. Simulation - helps explore different course  of Activations
2. Distributed Training - improve policy based on Simulation
3. Serve Policy - uses the trained policy to render an action based on the current state of the environment - aims to minimize latency, and maximize the number of decisions per second

- in RL all three of these workloads are tightly coupled in a single application, with stringent latency requirements between them and currently no framework supports all three of them.
- different frameworks can be stiched together for this but in practice its very hard to implement.

# Requirements for RL framework :-

1. support fine-grained heterogenous computations - duration of computations can vary drastically
2. must have a Flexible computation model - require both stateful and stateless computations
3. must support dynamic execution- results of a computation can determine future computations 

## ACME

- RL research now adays incorporates numerous heterogeneous components, contributing to their increased
complexity and to growing concerns about the reproducibility of research 
- Rl agent should be  able to recreate its own training data 
-  This motivates interacting with multiple instances of an environment (simulated or otherwise) in parallel to generate more experience to learn from.

# Acme - 
- software library and light-weight framework for expressing and training RL agents
- allowing for fast iteration of research ideas and scalable implementation of state-of-the-art agents
- Acme agents are easily scalable to large distributed systems,

# Modern reinforcement learning

- learning agent - an entity that perceives and acts— interacting with an unknown environment
- after interation agent receives reward for the observation
- agent’s goal is to maximize an aggregate of future rewards it expects to receive 
- learners - processes which consume data in order to update policy parameters,

Major challenges faced -

1. agent must explore its environment effectively
2. learn effectively from these experiences

## ORLEANS

Building interactive services that are scalable and reliable is hard.

The traditional three-tier architecture limits scalability. Caching layer is added to improve performance.
but cache loses most of the concurrency and semantic guarantees of the underlying storage layer

a stateless middle tier does not provide data locality since it uses the data shipping paradigm:
for every request, data is sent from storage or cache tothe middle tier server that is processing the request. 
problem solved with function shipping paradigm in actor model - has data locality and semantic and consistency benefits

USe of OOP - in actor  model , easier  to develop as  actors appear to developers  like the familiar model of interacting objects

Actor platforms such as Erlang and Akka - burden developers with many distributed system complexities because of the relatively low level of provided abstractions and system services. 

# Orleans Advantages -
1.  raises the level of the actor abstraction
2.  Orleans actor always exists, virtually. It cannot be explicitly created or destroyed.
3.  Orleans actors are automatically instantiated. Failiur is handled automatically. if server S catering to actor A crashes, A is automatically reisnstantiated on another server
4. the location of the actor instance is transparent to the application code, which greatly simplifies programming
5. Orleans can automatically create multiple instances of the same stateless actor, seamlessly scaling out hot actors

- the virtual actor approach significantly simplifies the programming model while allowing the runtime to balance load and recover from failures transparently

# Approaches

## MENGER

# SYSTEM DESIGN  

Menger uses local inference  ,  but pushes the scalability of actors to virtually an unbounded limit. 
main challenges to achieving massive scalability
1. large number of read requests from actors can throttle the learners
2. limitation of efficiency of input pipeline in feeding the training data to the TPU compute cores.

# Efficient Model Retrieval -

1. introducing caching components between the learner and the actors optimized in TensorFlow and backed by Reverb
2. They not only significantly reduces the pressure on the learner to service the read requests, but also further distributes the actors across multiple Borg cells
3. reduce the average read latency by a factor of ~4.0x leading to faster training iterations,

# High Throughput Input Pipeline  -

1. Menger uses Reverb, a recently open-sourced data storage system that provides an efficient and flexible platform to implement experience replay
As the number of actors grows the average write latency also increases significantly.
1.  menger  uses sharding capability by Reverb
2.  it balances  write  load from a large number of actors to multiple replay buffers
3. helps scale efficieantly

# Case Study: Chip Placement

1. Menger achieves significant improvements in the training time - 8.6 hr -> 1 hr
2. Menger has further potential 

## RAY

# Programming and Computation Model

# Programming Model

1. Tasks - represents the execution of a remote function on a stateless worker
         - When a remote function is invoked, a future representing the result of the task is returned
2. Actors - exposes methods that can be invoked remotely and are executed serially
          - runs on stateful set
          - handle can be to other actors so that the former actor can be called.
- Check Table R2 for Tasks vs Actors
-  To satisfy heterogenity API augmented in 3 ways

a. to handle concurrent tasks with heterogeneous durations, we introduce ray.wait(), which waits for the first k available results, instead of waiting for all results 
b.to handle resource-heterogeneous tasks, we enable developers to specify resource requirements so that the Ray scheduler can efficiently manage resources. 
c.  to improve flexibility, we enable nested remote functions, meaning that remote functions can invoke other remote functions

#  Computation Model
- Ray employs a dynamic task graph computation
model  in which the execution of both remote functions and actor methods is automatically triggered by the
system when their inputs become available

Computation Graph -
- 2 types of nodes - data objects and tasks
- 2 types of edges - data edges and control edges
- Data edge - dependency between data object and Tasks
- control edges - denote nested remote calls - task T1 invokes  T2
- Stateful edge - for actor method invocations (method M1 call M2)
- Using stateful edges can easily reconstruct lost data, whether produced by remote functions or actor methods

# Architecture

1. Application Layer

Processor types-
a. Driver: A process executing the user program
b. Worker: A stateless process that executes tasks (remote functions) invoked by a driver or another worker.
c. Actor: A stateful process that executes, when invoked, only the methods it exposes- needs to be explicitly instantiated

2. System Layer

3 components -

a.  Global Control Store (GCS)
- GCS is a key-value store with pubsub functionality.
-  maintain fault tolerance and low latency for a system that can dynamically spawn millions of tasks per second
- for Fault tolerence Ray decouples the durable lineage storage from the other system components, allowing each to scale independently.
- Involving the scheduler in each object transfer is prohibitively expensive and so Ray  store the object metadata in the GCS rather than in the scheduler, fully decoupling task dispatch from task scheduling
- it enables every component in the system to be stateless
- simplifies support for fault tolerance 
- makes it easy to scale the distributed object store and scheduler independently, as all components share the needed state via the GCS
- easy development of debugging, profiling, and visualization tools

b. Bottom-Up Distributed Scheduler
- scheduler needs to schedule millions of tasks per second 
- which is not in capacity of current centralized schedulers and distributed schedulers can but do notprovide data locality
- Ray designs 2 level hierarchical scheduler - global and per node local scheduler
- first task goes to local scheduler if i5t is overloaded it is forwarded to global scheduler.
- the global scheduler identifies the set of nodes that have enough resources of the type requested by the task, and of these nodes selects the node which provides the lowest estimated waiting time
- If the global scheduler becomes a bottleneck, we can instantiate more replicas all sharing the same information via GCS

3. In-Memory Distributed Object Store

- Ray implements the object store via shared memory on each  node
- For low latency, we keep objects entirely in memory and evict them as needed to disk in LRU  policy

# Step By Step Execution

1. Executing a task remotely
--- figure R3

2. Returning the result of a remote task
--- figure R4

# Evaluation

Microbenchmarks  -

1. Locality-aware task placement  - Actors, once placed, are unable to move their computation to large remote objects, while tasks can
                                  - tasks without data locality have 1-2 order more  latency

2. End-to-end scalability - near-perfect linearity in progressively increasing task throughput - scale linearly beyond 1.8 million tasks
per second at 100 nodes

3. Object store performance - the write throughput from a single client exceeds 15GB/s as object size increases
4. GCS fault tolerance -  lightweight chain replication layer built on top of redis to maintain low latency while
providing strong consistency and fault tolerance

5. GCS Flushing - after the GCS memory consumption reaches its capacity GCS flushing happens periodically
                - SO memory footprint is capped at a user-configurable leve
                - flushing mechanism provides a natural way to snapshot lineage to disk for long-running Ray applications

6. Recovering from task failures -  local schedulers reconstruct previous results in the chain in order to continue execution
                                 -  Overall per-node throughput remains stable throughout.

7. Recovering from actor failures - With minimal overhead, checkpointing enables only 500 methods to be re-executed, versus 10k re-executions without checkpointing
8. Allreduce - Ray completes allreduce across 16 nodes on 100MB in ∼200ms and 1GB in ∼1200ms, surprisingly outperforming OpenMPI (v1.10), a popular MPI implementation, by 1.5× and 2× respectively

# Building blocks

-  Here we isolate each of the workloads  - training, serving, and simulation to a setting that illustrates a typical RL application’s requirements

1.  Distributed Training

-  evaluate the performance of the Ray (synchronous) parameter-server SGD implementation against state-of-the-art implementations, using the same TensorFlow model
- Ray matches the performance of Horovod and is within 10% of distributed TensorFlow (in distributed replicated mode)

2.  Serving

- Ray focuses on embedded serving , Clipper foceses on serving predictions to clients
- Ray achieves an order of magnitude higher throughput for a small fully connected policy model that takes in a large input and is also faster on a more expensive residual network policy model
-  See Table R5 for actual comparison values

3. Simulation Comparison between -
- MPI implementation that submits 3n parallel simulation runs on n cores in 3 rounds, with a global barrier between rounds
- Ray program that issues the same 3n tasks while concurrently gathering simulation results 
- both systems scale well but Ray producess 1.8 x throughput as seen in Table R6

# RL Applications 

-  Ray’s programming model provides flexibility to  express application-level optimizations that would require lot of effort to port to other systems, but it is transparently supported by Ray’s dynamic task graph execution engine

1. Evolution Strategies
For large scale RL system evaluation ES algo which periodically broadcasts a new policy to a pool of workers and aggregates the results of
roughly 10000 tasks with  10 to 1000 simulation steps

- ray could scale up to 8192 cores as compared to 2048 cores at which the special purpose system failed
- Ray achieved a median time of 3.7 minutes, more than twice as fast as the best published result
- Initial parallelization of a serial implementation using Ray required modifying only 7 lines of code  as compared to hundreds  of  lines for other system

2. Proximal Policy Optimization
Ray compared to a highly-optimized reference implementation that uses OpenMPI communication primitives. 
- As shown in Figure 14b, the Ray implementation outperforms the optimized MPI implementation in all experiments, while using a fraction of the GPU
- Ray is heterogeneity-aware and allows the user to utilize asymmetric architectures easily whereas its  not the case with optimized implementation
- Ray’s fault tolerance and resource-aware scheduling together cut costs by 18×

# Related work

# Dynamic task graphs - ray closely related to CIEL and Dask
- CIEL provides lineage-based fault tolerance
- Dask fully integrates with Python
- Ray employs a fully distributed and decoupled control plane and scheduler, instead of relying on a single master storing all metadata
- With a centralized scheduler, each round of allreduce translates upto 2 x worse completion time

# Dataflow systems 
- other dataflow systems like MapReduce and Dryad are  too restrictive for a finegrained and dynamic simulation workload.
- Dryad releases ristriction a bit but does not support dynamic execution graphs
- They dont provide actor abstraction or implement a distributed scalable control plane and scheduler

# Machine Learning Frameworks

- eg. TensorFlow and MXNet 
- they provide limited support for the computation required to tightly couple training with simulation and embedded serving. 
- fault tolerence to be handled explicitly

# Actor systems

- orleans and Akka  provide less support for recovery from data loss
- in orlean statelful actors can be recovered by explicit checkpointing only
- in Akka does not provide efficient fault tolerance for stateless computation
- Orlean - at-least-once , Akka- at-most-once, Ray- exactly-once semanticsfor  message delivery

# Global control store and scheduling

- Ray uses bottom up scheduler
- generally other systems use centralized schedulers
- Sparrow uses decentralized but schedulers take independant decision limiting scheduling policy.
- Mesos uses 2 level scheduler but its global scheduler scheduler frameworks
- canary does not support for dynamic execution graphs but 
- Clik - work stealing scheduler - good performance but does noot support data locality.

## ORLEANS

# Programming Model

Actor - Actors are the basic building blocks of Orleans applications. An actor encapsulates behavior and mutable state, Actors are isolated, that is, they do not share memory. Thus, two actors can interact only by sending messages

Virtualization of actors

1. Perpetual existence - actors always exist, cant be created or destroyed,unaffected by the failure of a server that executes it.
2. Automatic instantiation:  An actor will not be instantiated if there are no requests pending for it.An unused actor’s in-memory instance is automatically reclaimed as part of runtime resource management.
3. Location transparency: actor may beinstantiated in different locations at different times, and sometimes might not have a physical location at all. 
4. Automatic scale out:  
a. ingle activation  mode (default), in which only one simultaneous activation of an actor is allowed
b. stateless worker  mode, in which many independent activations of an actor are created automatically by Orleans on-demand

# Actor interfaces - 
actors interact with each other through methods andproperties declared as part of their strongly-typed  interfaces
# Actor refrences  -  
is a strongly-typed virtual actor proxythat allows other actors, as well as non-actor code, to invoke methods and properties on it. are created locally by the sender and can immediately be used without a bind or register step
# Promises
Orleans method calls return immediately with a promise for afuture result, rather than blocking until the result is returned. Promises allow for concurrency without requiring explicit thread management.

Lifecycle state of promise -
1. unresolved-  awaiting  result in some  future
2. Fulfilled  - result received 
3. Broken - if error  occurs

Closure - scheduled when promise is resolved

# Turns -  
activations are single threaded and do work in chunks, called turns.

- by default , an activation does not receive a new request until all promises created during the processing of the current request have been resolved and all of their associated closures executed. Reentrant] attribute indicates that activation of that class may be given another request to process in between turns of a previous request.

# Persistance  - 
An actor class can declare a property bag interface that represents the actor state that should be persisted. RUntime  accordingly accounts for that.

# Timers and Reminders

2 types of timers -

1. transient timers- disapear when actor is deactivated
2.  Reminder - fores wether  or not actor is active . if that actor is inactive riminder causes activation call, also allow runtime to reclaim system resources by deactivating those actors

# Runtime Implementation

# Subsystems  -
1. Messaging - connects each pair of servers with a single TCP connection
2.  Hosting - where to place activations and manages their lifecycle
3.  Execution - runs actors’ application code on a set of compute threads

Process  of  actor Call  -

1.  E  converts call into message sends to M with identity of  target actor
2. M checks with H to determine target server.
3. H has distributed directory - fnids  either existing  activation or  creates new  activation.
4. M  serializes message sends to destination server.
5. if actor is busy message queued up
6. H  locally manages resources - if actor  idle for configurable time, its deactivated and claimed resourec are freed

# Distributed directory

- implemented as  one hop distributed Hash table.
-  Each server in the cluster holds a partition of the directory, and actors are assigned to the partitions using consistent hashing.
- extra hop required  to  find  physical  location of actor , so cache  maintained with recent resolved activation to actor mappings.

# Stroong Isolation 

- Actors can only talk through messages

Approached to reduce cost  of return values being deep copied  -
- immutable flag  to specify that it will not mutate an argument
- a highly optimized copying module that is part of the serialization subsystem in case of copy 

#Asynchrony

- promises represent future calls 
- Prevents application code from holding a thread while waiting for a result

# Single-Threading

- orlean ensures that max 1 thread runs in an activation
- this prevents race conditions or locks
- although this  affects performance but high parallelism covers for it.

# Cooperative Multitasking

- once started, an application turn runs to completion, without interruption
- helps run large number of activation on small number of threads
-  Orleans applications to run at very high CPU utilization

# Serialization

- Protocol Buffers , offer excellent performance at the cost of limiting the types of objects that may be passed.
- Orlean allows any data type and covers for performance by automatically generating custom
serialization code at compile time, with hand-crafted code

# Reliability

- orlean relieves developrs to explicitly manage aspects  of reliability except actors persistent  state
- O  has membership  mechanism for managing servers
-  Servers automatically detect failures via periodic heartbeats and reach an agreement on the membership view
-  when a server fails all data and activations on it are lost , all other servers get to know  about it and search through their directories to know activations on the failed server.
- next request to activation on failed server causes new  actiication to be created on surviving server.
- O does not impose a checkpointing strategy on wether or not an actor's state would be lost on server failure since its expensive
- checkpoints can also be done at a fixed time , itsup to the  developers
- if message is misdirected , the recipient either reroutes the message to the correct location or returns the message to the sender

# Eventual Consistency
- when failure occurs single activation for actor is ensured eventually.
- if membership is stale its possible that 2 activations of same actor are formed.
- one is dropped after membership settles
- done to ensure that applications can make
progress even when membership is in flux

# Messaging Guarantees

- at least once message delivery - resending messages after sometime
- Fifo ordering in message queues does not scale  well.
- use handshake, issuing a next call to an actor only after receiving a reply to the previous call.

# Applications

1. Halo 4 Presence service

- keeping track of all active game sessions, their participating players, and evolving game status. allows players to get added to a real time game
- Each game console  makes regular heartbeat calls to the service to report its status
- heartbeat message contains compressed session data including the unique session ID, the player IDs, and additional game data

# Structure

Actor  types-

1. Router - reccieves incoming heartbeats,  decompresses data,  forwards to session actor
2. Game session -  Session actor updates its internal in-memory state, calls player id 
3. Player -

--- pic figure O1

- Router has multiple activations , others have 0 or 1
- easy for developers to make calls in the code
- no need to write code to locate or instantiate the target actor

2. Halo 4 STatistics Service

-  processes results of completed and in-progress games with details of important events
- any statistics report posted to the service is initially pushed through a Windows Azure Service Bus [18] reliable queue, so that it can be recovered and processed in case of a server failure.
-  worker processes pull the requests from the queue and call the corresponding Game Session 
- The Game Session actor first saves the payload as-is to Azure BLOB store, then unpacks it and sends relevant pieces to Player actors
- Player actor then processes its piece and writes the results to Azure Table store. 
- Game actors in memory for the duration of the game and using caches reduces IO traffic to the storage and lowers latency.
-----figure 2

3. Galactic Reign services

-  turn-based, head-to-head game
- 4 types of actors
1. Stateful Game session - executes the game logic  -  results written-through to persistent Azure Storage.
                           holds cached copy of current game session
2. Vidoe Manager -  handles submission of jobs to the video rendering system
                    receiving notifications when render jobs are completed
                    forward completed rendering jobs to notification actors
3. notification actors - after game complete and video clips generated - notification actor sends message to game client
4. Housekeeper actor -  detect abandoned games and clean up the persisted game session data which is no longer needed.

4. Database Session Pooling

----- figure O3

Problems in accessing shared resource -

eg - using N databases with M shards - N X M connection ports , causes limitations - no. o frequired IP addresses exceeds range
- Instead orlean uses actor type named sharding
- these actors  used as proxies instead of dorect connections
- Advantage of using conection pool - they are reliable - reactivated automatically after failure

# Performance

1. Synthetic Micro Benchmarks

# Asynchronous IO and cooperative multi-tasking

- test uses 1000
actors and issues requests from multiple load generators
to fully saturate the system
- since  requests are asynchronous , calling thread  not blocked and very little  impact  on throughput of system.
--- figure O4

# Cooperative multitasking and threads

- throughput degrades with increase in number of threads due to extra cost of context switching, extra memory, longer OS scheduling
queues and reduced cache locality

# Price  of  Isolation

- since actors  are isolated , arguments in actor calls have to be deep copied
- price for  deep  copying is not that bad for simple data structures, but grows sugnificantly  for complex Orleans

2.  Halo Presence Performance Evaluation

# Scalability in the number of servers

- the throughput scales almost linearly as number  of servers
-  the throughput remains almost the same as the number of actors increases from 2 thousand to 2 million.

# Related work

comparison with Distributed Programming Frameworks

# Google App engine -
- both oop models
- GAE - model of Java/Python, synchronous RPC, multi Threading
- Orleans - single actor model , asynchronous RPC and singlethreading
- GAE has a built-in database  service 

# Enterprise Java Beans (EJB), Distributed Component Object Model (DCOM), and the Common Object Request Broker Architecture (CORBA) 

- generally provide synchronous communication
- they require static placement of objects
- None of them offers a virtual actor abstraction

# Actor Frameworks

-  O supports state encapsulation, safe message passing and location transperancy
-  weak mobility, supports best-effort fair scheduling

Erlang -

- has processes instead of actors
- actors  need to be expliciltly created in Erlang
- after creation location of process cant be changed
- this prevents dynamic load balancing across servers, actor migration, and automatic server failure-handling
- developer has to explicitly create the actor's Lifecycle

Akka -

- actor based, single threaded
- fifo ordering  between messages
- actor explicitly created
-  ability to load new code into an actor at runtime and a transaction mechanism, which ensures the effect of a set of actor invocations is atomic -  not in O

# Other actor Frameworks

- Kilim  focuses on single-node execution, and uses thread-switching for modeling actor execution and inter-actor communications. 
-ActorFoundry uses synchronous send/receive communication between actors instead of asynchronous, continuation-based APIs used in Orleans
-  Thorn  (and Erlang) use loosely-typed, dynamic actor interfaces which require care to match sender and receiver code - Monterey is an actor-based
- framework for Java . As in Orleans, an application
uses a key to obtain an actor reference. but requires  explicit management of actor  lifecycle

## ACME

# ACME - KEY Structural COntributions

1. Environments, actors, and environment loops

Environment -  with which an agent interacts - maintains its own state and is interacted with sequentially
Actor - actor consumes observations produced by the environment and produces actions that are in turn fed into the environment
Environment Loop - interation between agent and environment- genral one used

- interaction cycle between these 3 is shown in figure A1

2. Learners and Agents

Learner that consumes data to provide better policy
Agent - has both acting and learning  component 
      - agent triggers some number of learning steps within its learner component

Environment loop  for learning agent can be seen in -----figure A2 ------

3. Dataset and address

- dataset can be configured to hold on to stale data 
- actors can be programmed to add some noise
- Adders -  interface for insertion into the low-level storage system
- each actor  is able to by passits original component and  and  implement its  own actors
- Actors are of 2 types - feed forward and recurrent -  of how they maintain state

4. Distributed agents

- Acme splits acting, learning, and storage components into different threads
- this helps interactions to be asynchronous
- by using more actors in parallel we can speed up  learning  process

For  distributed variant

As shown in Figure A3
- each module is launched in its own
process, links are through remote procedure calls (RPC)
- in Figure A3 consists of data storage process, a learner process, environmental loop.
- variable client - allow the actor to poll the learner for variable updates and simplifies the code 

Tool used to implement distributed variant - Launchpad 

- mechanism for creating a distributed program as a graph consisting of nodes and edges
- nodes represent modules , edges represent communication channel between them 
- there is no difference between local and remote computation

5. Reverb and rate limitation

Reverb - 
- packaging , sampling and removing items from buffer can be easily configured
- by using rate limiter reverb enforces a desired relative rate of learning to acting 
- allowing the actor and learner processes to run unblocked
- if one is lagging behind it stops the other one to catch up within a given tolerence

6. Offline Agents
- it is trivial to apply any learning component to the offline setting
- since there is lot of overlap between pure batch setting and off-policy learning
- they perform very well without interacting with environment

# Agent Implementations

1. Background and notation

- interation between actor and environment uses markov decision process
- data expressed  in form of transitions - (ot , at ,rt , ot+1)- this keeos on iterating till end of episode signal is not true
- sub-slices  of  episodes - sequences

2.  Policy evaluation and value based agents
- using a neural network Qϕ to approximate Q π
- bootstrap target - one step of the backup operation introduced in the recursive Bellman equation
- online network parameters can be fit to this bootstrap target by minimizing the squared temporal difference (TD) error

Deep Q-Networks -
- DQN algo - actor simply chooses the action that has max value of Q function for a particular observation
- it is capable of learning from a policy   separate from the one it is optimizing
- uses n- step targets to use longer sequences of the observed reward signal

Recurrent DQN - 
-  makes use of a recurrent network, initialisation with a recurrent state S
- rather than learning from transitions R2D2 instead relies on full n-step sequences of the form
- R2D2 stores old sequence in the replay buffer with specified burn in periods because sequence of recurrent states is required.

Policy optimization and actor-critic agents -

- instead of maximising Q function , policy itself is directly parameterized by weights
- actor-critic learning paradigm -   learned value function  is learned in tandem with the policy 

Importance Weighted Actor-Learner Architecture (IMPALA)  -

- is a distributed advantage actor-critic agent which makes use of the loss.
- estimates state-value function using Monte Carlo rollout and minimizes error.
-  close to on policy algo
- designed to work in the distributed setting
- off policy correction and slowing down the policy  are key for actor-critic agents
- it introduces  entropy regularization to prevent instability caused by the policy moving too
quickly

# Continuous control -
- here the actions taken by the agent are real valued and therefore continuous
- here taking maximum value over a set of Q values is not possible 
- instead it alternates gradient steps optimizing the critic and policy loss

1. Deep Deterministic Policy Gradient - uses a deterministic policy with continuous actions.
                                      -  uses DQN with replaced learner and Gaussian exploration noise
2. Maximum a posteriori Policy Optimization (MPO) - 
- uses Kullback-Leibler (KL) divergence regularization which ensures 
- that the online policy does not move too far from the target network     
- that the online policy keeps adapting if necessary   

3. Distributional critics, D4PG, and DMPO -
- estimates the distribution over returns instead of expected return

#  Monte Carlo Tree search
- algorithm that figures out the best move out of a set of moves by Selecting → Expanding → Simulating → Updating the nodes in tree to find the final solution.

# Offline learning
- scenarios where interactions with the environment are expensive or dangerous  
-  it is not possible to directly use RL algorithms, which gather data while they learn— online learning
- learning algorithm is independent of the data generation process, making it amenable to both online
and offline learning
- Behaviour Cloning - agent learns to mimic the demonstrations by learning a mapping between observations and actions
via supervised learning

## Experiments  -

# Environments -
1. DeepMind Control suite - benchmark for continuous control algorithms 
                          - learning from raw features or pixels
                          - raw features - 3 to 137 dimensions
                          - pixels - consecutive RGB images of size 72 × 96
2. Arcade Learning Environment - simulator for 2600 Atari games
                               - observation space consists of 210 x 160 RGB image
3.  Behaviour Suite - studies the scalability and robustness of agents in various types of task

# Measurements Parameters -
1. Sample efficiency and actor steps
2. Speed and learner walltime

Results -

1. Results of Performance of control agents on the control suite from raw features comparing the single process
agents can be seen in figure A4.
2.  Results of performance camparison of D4PG comparing the single-process and two
distributed variants are given in figure A5
3. with a reasonable number of transitions, the task can be effectively learned offline and certainly off-policy
4. in the humanoid tasks, due to the complex composition of joints and the temporally extended nature of the tasks, accurately evaluating a state-action pair is tightly coupled with the policy being evaluated. 
5. combining a large replay buffer with a large SPI ratio, leads to a very off-policy learning
6.  using a relatively small replay buffer and SPI ratio, learning can be entirely on-policy

learning control suite tasks from pixel observations -  

1. when measured with respect to actor steps, performance and sample-efficiency is very
similar across the variants of the agent: single-process and distributed with 2, 8 and 16 actors
2. with respect to the learner’s walltime, the advantage of having more actors is clear
3. experiment shows that parallelism helps accelerate training, but that the same performance can still be
achieved with fewer actors and more time

Atari - 
- evaluate the performance of DQN, R2D2 and IMAPALA agents trained on 10 individual levels with 256  actors
- performance results are  shown in figure  A6

- Comparing aggregate performance of DQN, R2D2, MCTS and IMPALA on bsuite -- figure A7
- results of testing on 9 atari games show that the performance of our offline Acme DQN agent can
match the the best behavior policy that generated the data while being trained online -- figure A8

# Trade-Offs

## ACME

- notice that with no rate limitation at all, performance is similar to our default of 32 SPI. This is
due to our choice of number of distributed actors (4) in this experiment, which proved to be an efficient use of
computational resources. This can be investigated further phenomenon further in the future

## RAY

-Ray does not aim to substitute for serving systems like Clipper and TensorFlow Serving, as these systems address a broader set of challenges in deploying models, including model management, testing, and model composition. Similarly, despite its flexibility,
- Ray is not a substitute for generic data-parallel frameworks, such as Spark, as it currently lacks the rich functionality and APIs (e.g., straggler mitigation, query optimization) that these frameworks provide.
- such a framework is not intended for implementing deep neural networks or complex simulators from scratch. Instead, it should enable seamless integration with existing simulators

## ORLEAN

-  ability to load new code into an actor at runtime and a transaction mechanism, which ensures the effect of a set of actor invocations is atomic is not present in O
- A weakness of cooperative multitasking is that a poorly behaved component can take up an entire processor, degrading the performance of other components.

# Open Questions and Future Work

## ACME 
- by simply adding a residual network (ResNet) torso to the exact same network architecture as above, many complex tasks were learned without any additional tuning. Naturally, there is still a lot of room for improvement as many of the tasks are not learned
- The details of those offline  datasets will be available in an upcoming offline RL benchmark dataset release as RL Unplugged. RL Unplugged will include a diverse set of challenging Acme compatible offline RL datasets along with a proper evaluation method for each dataset. 

## RAY

- For simplicity, object store does not support distributed objects, i.e., each object fits on a single node. Distributed objects like large matrices or trees can be implemented at the application level as collections of futures.
- In the future, we hope to further reduce actor reconstruction time, e.g., by allowing users to annotate methods that do not mutate state
-  Scheduling optimizations in Ray might require more complex runtime profiling.
- storing lineage for each task requires the implementation of garbage collection policies to bound storage costs in the GCS which is being developed

## ORLEAN

- certain patterns do not fit Orleans well. One such pattern is an application that intermixes frequent bulk operations on many entities with operations on individual entities.
- Orleans does not yet support crossactor transactions, so applications that require this feature outside of the database system are not suitable
