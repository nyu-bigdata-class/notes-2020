---
title: "Applications: Reinforcement Learning"
author: Kevin Choi <kc2296@nyu.edu>
---
# Introduction and Motivation
As reinforcement learning (RL) has paved the road for many groundbreaking applications in the recent years, it has been getting more important than ever to be able to cater to RL model training, serving, and simulation requiring high throughput and low latency as well as flexibility and fault tolerance.

This week's papers elaborate on four approaches:
1. Orleans (Bernstein et al. from Microsoft Research)
2. Ray (Moritz et al. from UC Berkeley)
3. Acme (Hoffman et al. from DeepMind)
4. Menger (Yazdanbakhsh et al. from Google AI)

# Approaches and Trade-Offs
## Orleans
Orleans can be thought of as a distributed .NET framework. While not designed specifically with RL in mind, Orleans is one of the first substantial realizations (in addition to Erlang and Akka) of the actor model -- a conceptual concurrent computation model in which each actor unit can carry out local decisions via its own private state and messages it receives from other actors and can interact with other actors by creating them or sending messages to them -- which aligns with the aspects of RL. The main contribution of Orleans comprises the virtual actor abstraction.

Novel in that an actor is not tied to a physical space, the virtual actor abstraction helps solve a number of complications related to distributed systems such as reliability and distributed resource management. The following four facets pertaining to virtualization of actors in Orleans are notable:
1. Perpetual existence. As actors are purely logical entities, they always exist (virtually).
2. Automatic instantiation. The Orleans runtime instantiates an actor automatically when a new request is sent to an actor that is not yet instantiated.
3. Location transparency. An actor may be instantiated in different locations at different times (or might not have a physical location at all). One could think of this as texting a phone number.
4. Automatic scale-out. In addition to the default single activation mode, stateless worker mode allowing many independent activations of an actor to increase throughput is supported.

Other notable aspects of Orleans include the following:
* As actors are virtual, no actor fails (a la fault tolerance) when a server fails.
* Orleans allows an asynchronous programming style, as promises are used extensively to represent future results.
* Availability is preferred over consistency (out of CAP).
* The actor model upholds developer-friendliness via bringing OOP back to the system level.
* Persistence management is provided simply via property bag interface.

## Ray
Ray implements a unified interface that expresses both task-parallel and actor-based computations. In other words, it benefits from stateless computations (via tasks) as well as stateful computations (via actors) with an aim to achieve the following: fine-grained computations, heterogeneity both in time and resource usage, and dynamic execution.

Ray's architecture is composed of an application layer implementing the API and a system layer providing high scalability and fault tolerance.
1. The application layer comprises 3 types of processes. Driver is a process executing the user program. Worker is a stateless process executing tasks. Actor is a stateful process.
2. The system layer comprises a global control store (GCS), a distributed scheduler, and a distributed object store. As the object metadata is stored in the GCS rather than the scheduler, a decoupling of task dispatch and task scheduling is made possible achieving low latency. Also notable is the principle of bottom-up scheduling where scheduling is done via two-level hierarchy consisting of a global scheduler (submitted to later) and per-node local schedulers (submitted to first).

## Acme
Acme is a research framework for distributed reinforcement learning, designed to enable simple agent implementations that can be run at various scales of execution. Providing tools at various levels of abstraction (e.g. from networks, losses, and policies to actors and learners to experimental apparatus including loops, logging, and checkpointing), Acme accomplishes distributed computations by splitting the acting, learning, and storage components into different threads or processes and by allowing local inference per distributed agent, from which replay and update components are moved externally.

## Menger
Menger improves upon Acme's methodology of local inference. More generally, Menger is large-scale distributed RL infrastructure with localized inference that scales up to thousands of actors across multiple processing clusters and addresses two challenges: servicing a large number of read requests from actors to a learner for model retrieval and efficiency of the input pipeline when feeding the training data to the TPU compute cores.

Solutions to the two challenges:
1. Transparent and distributed caching components between the learner and the actors are optimized in TensorFlow and backed by Reverb.
2. Employed is the sharding capability provided by Reverb to increase the throughput between actors, learner, and replay buffer services.

# Open Questions and Future Work
## Orleans
While the Orleans programming model works for many applications, it is not well-suited for an application that involves frequent bulk operations on many entities as well as operations on individual entities. The reason is that isolation of actors makes such bulk operations expensive.

## Ray
Ray does not address the broadest set of challenges in deploying models (e.g. model management, testing, and model composition). Additionally, Ray, although flexible, is not meant to compete with generic data-parallel frameworks like Spark due to its lack of rich APIs (e.g. query optimization).

## Acme and Menger
As Menger's applicational discussion revolves around applying RL to chip design process, it would be relevant to explore how Menger might perform in other tasks in RL.