---
title: "Applications: Reinforcement Learning"
author: Kevin Choi <kc2296@nyu.edu>
---
# Introduction and Motivation
As reinforcement learning (RL) has paved the road for many groundbreaking applications in the recent years, it has been getting more important than ever to be able to cater to (parallel/asynchronous) RL model training, serving, and simulation requiring high throughput and low latency as well as flexibility and fault tolerance.

This week's papers elaborate on four approaches:
1. Orleans (Bernstein et al. from Microsoft Research)
2. Ray (Moritz et al. from UC Berkeley)
3. Acme (Hoffman et al. from DeepMind)
4. Menger (Yazdanbakhsh et al. from Google AI)

# Approaches and Trade-Offs
## Orleans
Orleans can be thought of as a distributed .NET framework. While not designed specifically with RL in mind, Orleans is one of the first substantial realizations (in addition to Erlang and Akka) of the actor model -- a conceptual concurrent computation model in which each actor unit can carry out local decisions from its own private state and messages it receives from other actors and can interact with other actors by creating them or sending messages to them -- which aligns with the aspects of RL. The main contribution of Orleans comprises the virtual actor abstraction.

Novel in that an actor is not tied to a physical space, the virtual actor abstraction helps solve a number of complications related to distributed systems such as reliability and distributed resource management. The following four facets pertaining to virtualization of actors in Orleans are notable:
1. Perpetual existence. As actors are purely logical entities, they always exist (virtually).
2. Automatic instantiation. The Orleans runtime instantiates an actor automatically when a new request is sent to an actor that is not yet instantiated.
3. Location transparency. An actor may be instantiated in different locations at different times (or might not have a physical location at all). One could think of this as texting a phone number (and the recipient could be anywhere).
4. Automatic scale-out. In addition to the default single activation mode, stateless worker mode allowing many independent activations of an actor to increase throughput is supported.

Other notable aspects of Orleans include the following:
* As actors are virtual, no actor fails (a la fault tolerance) when a server fails.
* Orleans allows an asynchronous programming style via promises that are used extensively to represent future results.
* Availability is preferred over consistency (out of CAP) such that Orleans is AP.
* The actor model upholds developer-friendliness via bringing OOP back to the system level.
* Persistence management is provided simply via property bag interface that can be declared.
* The Orleans distributed directory is implemented via one-hop distributed hash table.

## Ray
Ray implements a unified interface that expresses both task-parallel and actor-based computations. In other words, it benefits from stateless computations (via tasks) as well as stateful computations (via actors) with an overall aim to achieve the following: fine-grained computations (e.g. supporting actions in milliseconds while interacting with the real world environment as well as performing simulations), heterogeneity both in time (seconds vs hours) and resource usage (CPUs vs GPUs), and dynamic execution.

Ray's architecture is composed of an application layer implementing the API and a system layer providing high scalability and fault tolerance.
1. The application layer comprises 3 types of processes. Driver is a process executing the user program. Worker is a stateless process executing tasks. Actor is a stateful process.
2. The system layer comprises a global control store (GCS), a distributed scheduler, and a distributed object store. As the object metadata is stored in the GCS rather than the scheduler, a decoupling of task dispatch and task scheduling is made possible achieving low latency. To add to scalability and fault tolerance, the GCS makes use of sharding and per-shard chain replication. In terms of scheduling, what is notable is the principle of bottom-up scheduling furthering scalability where scheduling is done via two-level hierarchy consisting of a global scheduler (submitted to later) and per-node local schedulers (submitted to first). Also implemented to minimize task latency is an in-memory distributed storage system to store the inputs and outputs of every task.

Utilizing a dynamic task graph computation model (involving data edges, control edges, and stateful edges), Ray is able to provide both an actor and a task-parallel programming abstraction. The paper's evaluation boasts scalability up to 1.8 million tasks per second and transparent fault tolerance.

## Acme
Acme is a research framework for distributed reinforcement learning, designed to enable simple agent implementations that can be run at various scales of execution. Providing tools at various levels of abstraction (e.g. from networks, losses, and policies to actors and learners to experimental apparatus including loops, logging, and checkpointing), Acme accomplishes distributed computations (for parallelism and asynchrony) by splitting the acting, learning, and storage components into different threads or processes and thus by allowing local inference per distributed agent.

Reverb (a low-level data storage system) is used to support a replay buffer system. Some algorithms built into Acme include Deep Q-Networks (DQN), recurrent DQN (R2D2), IMPALA, Deep Deterministic Policy Gradient (DDPG), maximum a posteriori policy optimization (MPO), distributional critics, and Monte Carlo tree search.

## Menger
Menger improves upon Acme's methodology of local inference. More generally, Menger is a large-scale distributed RL infrastructure with localized inference that scales up to thousands of actors across multiple processing clusters and addresses two challenges: servicing a large number of read requests from actors to a learner for model retrieval and efficiency of the input pipeline when feeding training data to TPU compute cores.

Solutions to the two challenges:
1. Transparent and distributed caching components between the learner and the actors are optimized in TensorFlow and backed by Reverb.
2. Employed is the sharding capability provided by Reverb to increase the throughput between actors, learner, and replay buffer services.

# Open Questions and Future Work
## Orleans
1. While the Orleans programming model works for many applications, it is not well-suited for an application that involves frequent bulk operations on many entities as well as operations on individual entities. The reason is that isolation of actors makes such bulk operations expensive.

## Ray
1. Ray does not address the broadest set of challenges in deploying models (e.g. model management, testing, and model composition).
2. Ray, although flexible, is not meant to compete with generic data-parallel frameworks like Spark due to its lack of rich APIs (e.g. query optimization).
3. Scheduling in Ray is not as optimized as it can be.

## Acme and Menger
1. As Menger's applicational discussion revolves around applying RL to chip replacement, it would be relevant to explore how Menger might perform in other tasks in RL.