---
Title: "Introductions and Trends"
Author: Vidit Bhargava <vvb238@nyu.edu>
---

## Dandelion

#### Introduction
Dandelion aims to provide a single simple programming interface for running demanding applications on heterogenous clusters containing CPUs, GPUs, FPGAs etc while abstracting away parallelizing and mapping computation. 

#### Motivation
Heterogenous systems gain performance or energy efficiency not just by adding the same type of processors, but by adding dissimilar coprocessors, usually incorporating specialized processing capabilities to handle particular tasks. Dandelion is designed for compute clusters where each node is equipped with multi-core CPUs and GPUs. These clusters are an attractive and affordable computing platform for many demanding applications. Thus, Dandelion has to manage multiple execution contexts with very different programming abstractions and runtimes. This includes handling the complexities such as
1. Architectural heterogeneity - Different instruction-sets/OS services/Application binary interface
2. Resource scheduling - Integrating multiple runtimes efficiently to enable high performance for the overall system.
3. Concurrency - Synchronization and consistency 
To keep things simple, it allows the programmer to write sequential code in a high-level programming language such as C#, and the system automatically executes it utilizing all the parallel compute resources available.

#### Approach
##### Input
Dandelion takes in the user program written in LINQ (Language Integrated Query) as well as the partitioned data files. LINQ provides relational operators on collections. The operators are integrated seamlessly into high-level .NET programming languages

##### Compiler
Firstly, it generates CUDA code by performing translation on .NET byte-code by mapping C# types to CUDA structs and methods to kernel functions. The main constraint lies in dynamic memory allocation as GPU dont deal with it well. Secondly, it generates three levels of dataflow graphs to orchestrate the execution
1. Cluster Level - optimized for parallel execution on the distributed compute substrate. Decides which machine executes what.
2. Machine Level - describing the execution on that machine and how to manage input/output and execution threads
3. GPU level - generated to describe the computation.


##### Runtime 
Cluster execution engine assigns vertices to available machines and distributes code and graphs, orchestrating the computation. Each node executes its own dataflow graph, managing input/output and execution threads.
1. GPU Dataflow Engine (PTask) - DAG-based dataflow execution engine with constructs that can be composed to express iterative structures and data-dependent control flow; these constructs are required to handle cyclic dataflow and streaming for Dandelion
2. Cluster Dataflow Engine - 
    1. Dryad - Large cluster where fine grained fault tolerance is required. This design, the output of a vertex is written to disk files so transient failures can be recovered by vertex re-execution using its disk inputs.
    2. Moxie - allows the entire computation to stay in memory when the aggregate cluster memory is sufficient. holds intermediate data in memory and can checkpoint them to disk. caches in memory datasets (including intermediate data) that will be used multiple times in a single job.
3. Machine Dataflow Engine - Asynchronous channels are created to transfer data between the CPU and GPU memory spaces
    1. For CPU: parallelize the computation on multi-core
    2. For GPU: dispatch computation to GPU dataflow engine

#### Trade-offs
1. Dandelion can improve the performance of computations on the GPU when it knows ahead of time that it is operating on a sequence of fixed-length records. Dandelion depends on low-level GPU run-times such as CUDA. These runtimes have very limited support for device-side dynamic memory allocation, and even when it is supported, the performance can be poor.
2. All the user-defined functions invoked by the LINQ operators must be side-effect free, and Dandelion makes this assumption about user programs without either static or runtime checking.
3. Difficulty of handling variable-length records. To address this problem, Dandelion treats a variable-length record as an opaque byte array, coupled with metadata recording the total size of the record and the offset of each field. However, the over-head of the metadata and additional wrappers functions can cause performance problems. Dandelion resorts to general mapping only when it fails to statically infer the size of the record type.
4. Dandelion is able to improve performance using parallel hard-ware in all cases, the end-to-end speedup can erode sig- nificantly due to I/O. Moreover, the degree to which GPU parallelism is profitable relative to CPU parallelism varies significantly as a function of Dandelionâ€™s ability to hide data marshalling, transfer, and memory management latencies with device-side execution
5. Some tests also show lesser performance on a distributed cluster; which can be attributed to the overhead associated with read from disk files, deserialized to CPU memory, and then transferred to GPU memory.

#### Open Questions and Future Work
One apparent disadvantage to this system is the adherence to LINQ. Moreover, the work is promising but incomplete because it is tested on limited number of workflows and accelerators. 

Another limitation of a batch dataflow system is that it requires the input data to be immutable, and all of the subcomputations to be deterministic, so that the system can re-execute subcomputations when machines in the cluster fail.

If one of the major contributors to performance is I/O. Would this benefit from a query planning?


## Windows Azure Storage
#### Introduction
Windows Azure Storage enables the ability to store data in the cloud without any size or time limitations while paying for only for what is used and stored. It is responsible for managing the replication and data placement across the disks and load balancing the data and application traffic within the storage cluster.

#### Motivation
The key design goals for this paper were:
1. Strong Consistency - Ability to perform conditional read, writes and deletes for optimistic concurrency control (multiple transactions complete without interfering with each other) on strongly consistent data.
2. Scalability / Global and Scalable Namespace - Store and acccess massive(exabyte = 1 billion GB) data from any location. This is allowed by breaking the namespace into three parts and accessing it by ```http(s)://AccountName.<service>1.core.windows.net/PartitionName/ObjectName```. 
    1. Account name - part of DNS host name and is used to locate the primary storage cluster.
    2. Partition name - locate data once the request reaches the cluster.
    3. Object name - identifies individual objects within the partition.
3. High Availability / Disaster Recovery - Achieved by storing data across multiple data centers hundreds of miles apart
4. Multi-tenancy and Cost of Storage - low cost by sharing infrastructure 
#### Approach
Three data abstractions - 
1. Blobs (User files)
2. Tables (Structured storage)
3. Queues (Message delivery)

Each blob name is the partition name, whereas the primary key for a table is a combination of object name and partition name. Queue names are partition names and each message is referred by object name.

High Level Architecture 
1. Storage Stamps - Goal is to use around 70% in terms of capacity transactions and bandwidth. Leaving around 20% for short stroking (placing data on outer sectors of the disk for better sek times) and continuing to provide storage capacity if a rack within a  stamp fails.
    1. Stream layer - Append-only distributed file system layer. Each file is replicated three-times across different domains. Re-replicating on failure or checksum mismatch. (Paxos)
    2. Partition Layer - handles transaction semantics, caching and strong consistency for blob, table and queues. Stores and reads objects to/from Stream layer
    3. Front end layer - Stateless servers that authenticate, authorize and route requests.
2. Location Services - allocates accounts to storage stamps and manages them across the storage stamps for disaster recovery and load balancing. To provision additional capacity, the LS has the ability to easily add new regions, new locations to a region, or new stamps to a location. 
3. Replication Service
    1. Inter-stamp - Synchronous replication for durability within the stamp across nodes. Focus on objects and their transactions
    2. Intra-stamp - Asynchronous replication across stamps to enable disaster recovery. Focus on Blocks of disk storage
4. Scalable Object Index - Hundreds of billions of blobs across accounts can be stored in a single stamp. It is important to efficiently and scalably enumerate and query them even when traffic patterns are erratic (hot objects/peak loads/traffic bursts). Partition Layer maintains an object index table for each data abstraction and monitors load to each part to determine hot spots. If the load is high, index is split into thousands of range partitions dynamically and load balanced across servers.
#### Trade-offs
#### Open Questions and Future Work

## TPU
#### Introduction
The recent successes of Neural networks have prompted new innovation in Domain Specific Architectures. TPUs are different from traditional CPUs because they have fewer but larger cores with each one having 128x128 or 256x256 [systolic arrays](http://web.cecs.pdx.edu/~mperkows/temp/May22/0020.Matrix-multiplication-systolic.pdf) of multipliers. TPUs deal with narrower data to improve efficiency and do not utilize caches and branch predictors.
Each neural network has two phases; training and inference. Both include common elements like Matrix multiplications, convolutions and activation functions. However, training requires more computation and memory while also being harder to parallelize. The demand training computation appears to be unlimited; thus, justifying the need for its own custom computer.  
#### Motivation
During the time of this paper, training took place on clusters of CPUs; primarily through Asyncronous Stochastic Gradient Descent which involves communication between parameter servers and workers. Building a super computer was necessary because at-that-time training times were huge and neural networks had a tendency to perform better when trained on bigger datasets.
#### Approach
#### Trade-offs
#### Open Questions and Future Work





## Trade-Offs
Compare the approaches, discuss when they might be appropriate. For example,
some techniques might only make sense with a lot of data, or in the absence of
multiple tenants, etc.

## Open Questions and Future Work
How are the current trends affecting the area, what are some open questions
about the problem, etc.


